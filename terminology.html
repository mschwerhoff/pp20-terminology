<!DOCTYPE html>
<html>
<head>
    <link href="milligram.css" rel="stylesheet">
    <link href="custom.css" rel="stylesheet">
</head>
<body>
<div class="wrapper">
  <div id="content" class="container">
    <h1>Parallel Programming (1st half): Terminology</h1>
<dl>
<dt>atomic</dt>
<dd>A statement or instruction is (truly) atomic if it is executed by the CPU in a single, non-interruptible step.</dd>
<dt>abstractly atomic</dt>
<dd>A statement or instruction that, at a certain level of abstraction, appears to be executed atomically. E.g. from a caller's perspective, a method <code>synchronized append(x)</code> of a queue appears to append element <code>x</code> in one step, but from the queue's perspective, this might take several steps.</dd>
<dt>bad interleaving</dt>
<dd>An interleaving that yields a problematic or otherwise undesirable computation. E.g. an incorrect result, a deadlock or non-deterministic output.</dd>
<dt>busy waiting</dt>
<dd>Occurs when a thead busily (actively) waits, e.g. by spinning in a loop, for a condition to become true. In the opposite scenario, the thread sleeps (i.e. is blocked; in Java: <code>join()</code>, <code>wait()</code>) until the condition becomes true. Trade-off: busy waiting uses up CPU time, whereas blocking may cause additional context switches.</dd>
<dt>cache coherence protocols</dt>
<dd>Hardware protocols that ensure consistency across caches, typically by tracking which locations are cached, and synchronising them if necessary.</dd>
<dt>CISC, RISC</dt>
<dd>CISC (complex instruction set computer) and RISC (reduced instruction set computer) are two fundamental CPU architecture models. Classical RISC is easier to study, since simpler: e.g. RISC instructions can only work on registers, and reading/writing memory are separate instructions.</dd>
<dt>concurrency</dt>
<dd>Parallelism that requires reasoning about and managing shared resources. Often used interchangeably with parallelism.</dd>
<dt>context switch</dt>
<dd>Given a computation unit (CPU), a context switch denotes the action of switching the unit from one computation to another. Typically refers to switching between processes, but can also refer to switching between threads. Depending on the size of the context ("large" for a process, "small" for thread), a context switch might be computationally expensive, i.e. require comparably much CPU time.</dd>
<dt>critical section</dt>
<dd>A piece of code that, in order to guarantee correct program execution, may only be executed by one thread at a time.</dd>
<dt>data race</dt>
<dd>A program has a data race if, during any possible execution, a memory location could be written from one thread, while concurrently being read or written from another thread. Data race is often used interchangeably with race condition.</dd>
<dt>deadlock</dt>
<dd>Circular waiting/blocking (no instructions are executed/CPU time is used) between threads, so that the system (union of all threads) cannot make any progress anymore.</dd>
<dt>functional unit</dt>
<dd>A component of a CPU (or core) that performs a certain task, e.g. executing integer arithmetic operations. An execution unit is one such a functional unit, see also RISC.</dd>
<dt>granularity</dt>
<dd>Coarse vs. fine: Splitting work into large tasks (coarse) reduces overhead, but might not use all available threads. Small tasks (fine granular) can be parallelized more, but also add more overhead. The trick is to find a "reasonable" size to minimize overhead and maximize parallelism.</dd>
<dt>instruction level parallelism</dt>
<dd>CPU-internal parallelisation of independent instructions, with the goal of improving performance by increasing utilisation of a CPU's functional units.</dd>
<dt>interleaving</dt>
<dd>Given multiple threads, each executing a sequence of instructions, an interleaving is a sequence of instructions obtained from merging the individual sequences. A sequentially consistent interleaving is one where the relative order of statements from one thread is preserved.</dd>
<dt>latency</dt>
<dd>An evaluation metric for pipelines. Latency measures the time a pipeline needs to process a given work item (e.g. a CPU instruction).</dd>
<dt>livelock</dt>
<dd>A situation in which all threads starve by infinitely often try to enter a critical section, but never succeeding. Similar to a deadlock, the system makes no real progress, although the threads execute statements/use CPU time.</dd>
<dt>liveness property</dt>
<dd>Property of a system: "something good eventually happens". Can only be violated in infinite time. Infinite loops and starvation are typical safety properties. Will be formally defined in Formal Methods using temporal logic.</dd>
<dt>locality</dt>
<dd>
<p>Has several meanings in the context of parallel programming:</p>
<ol>
<li>Locally reason about one thread at a time (also known as thread modularity). Simplifies correctness arguments.</li>
<li>Data locality: related memory locations are accessed shortly after each other. Improves performance by optimal cache usage.</li>
<li>Code locality: straight-line code increases opportunities for instruction level parallelism</li>
</ol>
</dd>
<dt>lock</dt>
<dd>In general, a token/resource that can be acquired by at most one thread at a time. Locks are typically provided by a programming language to enforce mutual exclusion, by guarding/protecting a critical section. A lock can be acquired/locked by a thread, and is then held until it is released/unlocked. In Java, each object can be used as a lock (intrinsic/monitor lock), but the JDK also provides more complex locks.</dd>
<dt>lockout</dt>
<dd>Needlessly preventing a thread from entering a critical section.</dd>
<dt>mutual exclusion</dt>
<dd>Preventing more than one thread from being in a critical section, i.e. to execute a piece of code, at a given moment in time.</dd>
<dt>multiprocessing (multitasking)</dt>
<dd>Typically refers to parallelism on the operating system level, i.e. to processes running in parallel.</dd>
<dt>multithreading</dt>
<dd>Threads running in parallel.</dd>
<dt>parallelism</dt>
<dd>Performing computations simultaneously; either actually, if sufficient computations units (CPUs, cores, ...) are available, or virtually, via some form of alternation. Often used interchangeably with concurrency. Parallelism can be specified explicitely by manually assigning tasks to threads or implicitely by using a framework that takes care of distributing tasks to threads.</dd>
<dt>parallel execution time</dt>
<dd>$T_p$. The time that is required to perform some work on $p$ processors.</dd>
<dt>process</dt>
<dd>Independently running instance of a program/application, typically on the operation system level. Similar to a thread, but usually more heavy-weight (since a whole program) and encapsulated in memory.</dd>
<dt>process context</dt>
<dd>All state associated with a process, including CPU state (registers, program counter), program state (stack, heap, resource handles), and additional management information. A thread also has a context, but it is typically much smaller.</dd>
<dt>race condition</dt>
<dd>A program has a race condition if, during any possible execution with the same inputs, its observable behaviour (results, output, ...) may change if events happen in different order. Events here are typically scheduler interactions causing different interleavings, but could also be, e.g. changing network latency. Race condition is often used interchangeably with data race.</dd>
<dt>reentrancy</dt>
<dd>A lock is reentrant if it can be acquired (and released) multiple times by the same thread. If a lock is non-reentrant, trying to acquire it again might cause an exception or other problems.</dd>
<dt>safety property</dt>
<dd>Property of a system: "nothing bad ever happens". Can be violated in finite time. Exceptions, absence of deadlocks, and mutual exclusion are typical safety properties. Will be formally defined in Formal Methods using temporal logic.</dd>
<dt>sequential execution time</dt>
<dd>$T_1$. The time that is required to perform some work on a single processor.</dd>
<dt>scalability</dt>
<dd>In our context: By how much can a program be parallelized. What is the maximum speedup that can be achieved, given an infinite amount of processors. See "speedup".</dd>
<dt>scheduler</dt>
<dd>A management process, e.g. on the operating system level, that performs context switches. I.e. it interrupts/pauses/sends to sleep the currently running process (or thread), performs a context switch, and selects the next process (or thread) to run. Schedulers typically do not give guarantees when and how often they act, who gets selected next, etc.</dd>
<dt>shared resource</dt>
<dd>Any resource (memory location, input source, output sink, ...) shared by more than one thread.</dd>
<dt>starvation</dt>
<dd>A thread starves if it can never enter a/any critical section.</dd>
<dt>synchronisation</dt>
<dd>Some form of orchestration via threads. Typically, to prevent bad interleavings.</dd>
<dt>synchronized</dt>
<dd>Java keyword, enforcing mutual exclusion for a critical section via some object's intrinsic lock.</dd>
<dt>throughput</dt>
<dd>An evaluation metric for pipelines. Throughput measures the amount of work (e.g. CPU instructions) that can be done by a pipeline in a given period of time.</dd>
<dt>thread</dt>
<dd>In general, an independent (i.e. capable of running in parallel) unit of computation that executes a piece of code. The concept of threads exists on various levels: hardware (CPU), operating systems, programming languages. In Java, also an instance of the <code>Thread</code> class.</dd>
<dt>thread mapping</dt>
<dd>How a Java/JVM thread is related to an operating system thread. In native threading (most common), each JVM thread is mapped to a dedicated operating system thread. In green threading, the JVM maps several threads to a single operating system thread.</dd>
<dt>vectorisation</dt>
<dd>Using special machine code instructions to execute a single operation (e.g. plus) on a chunk of data (e.g. an array segment). Can significantly improve performance. Code can be vectorised automatically, by compilers, or manually, by using intrinsics libraries provided by hardware vendors.</dd>
<dt>work partitioning</dt>
<dd>Split-up of a program into smaller tasks that can be executed in parallel. Ideally, each task performs its work independently of any other task, for instance on separate areas of a data structure.</dd>
</dl>
  </div>
  <div id="footer" style="line-height: 0; padding-top: 0.5rem; padding-bottom: 0.5rem;">
    <span class="centered-vi">
      <span class="svg-markdown" style="width: 18px; height: 18px; margin-right: 5px;">&nbsp;</span>
      Last updated 2020-03-30 16:05 GMT
      <span class="separator-h">&nbsp;</span>
      <span class="svg-git" style="width: 20px; height: 20px;">&nbsp;</span>
      <a href="https://github.com/mschwerhoff/pp20-terminology/commit/c16668aee7b567cdc9ee59fed3f325f566862f6b">c16668aee7</a>
      <span class="separator-h">&nbsp;</span>
      <span class="svg-github" style="width: 16px; height: 16px; margin-right: 4px;">&nbsp;</span>
      <a href="https://github.com/mschwerhoff/pp20-terminology">Sources</a>
    </span>
  </div>
</div>
</body>
</html>